<!DOCTYPE html>
<html>
  <head>
    <link rel="stylesheet" type="text/css" href="styles.css" />
    <link
      href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&display=swap"
      rel="stylesheet"
    />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>
    <title>EECS106 A Final Project</title>
  </head>

  <body class="pageContents">
    <nav class="top-bar">
      <a href="index.html" class="nav-item">Introduction</a>
      <a href="design.html" class="nav-item">Design</a>
      <a href="implementation.html" class="nav-item">Implementation</a>
      <a href="results.html" class="nav-item">Results</a>
      <a href="conclusion.html" class="nav-item">Conclusion</a>
      <a href="team.html" class="nav-item">Team</a>
      <a href="additional.html" class="nav-item">Additional Materials</a>
    </nav>
    <div class="background">
      <h1 class="backgroundImageTitle">
        EECS 106A Final Project - Grippy Kasparov
      </h1>
      <p class="backgroundImageText">EECS106A Fall 2024 Team 23</p>
    </div>
    <h2 class="title">Implementation</h2>
    <div class="wrapper">
      <div class="content">
        <h3>Hardware:</h3>
        <h4>UR10 Robot Arm:</h4>
        <p>
          We are really lucky that the Embodied Dexterity allowed us to use
          their UR10 robot arm to finsih our project. Although there are some
          difference between the codes and functions from what we used in the
          lab for a sawyer, but the idea is generally similar that is easy for
          us to adapt. The only stuggle is that we need to get used to
          initializing rtde (real time data exchange) helpler before we call or
          perform any function or action about the robot arm.
        </p>
      </div>
      <img class="noteImage" src="images/ur10.png" />

      <div class="content">
        <h4>Circuit:</h4>
        <p>
          The smart suction cup originally only have negative pressure to suck
          item up. In thid case, the placing action of the robot arm will be
          hard since it takes some time for the item to fall off even after
          turning off the suction cup. So we design and create another circuit
          that can create positive pressure to push the item off instead off
          letting it fall naturally. Hers is a <a href="">link</a> to the detail
          circuit diagram in the additional item section.
        </p>
      </div>

      <div class="content">
        <h4>Smart Suction Cup:</h4>
        <p>
          The idea of smart suction cup is to pick up items with complex shape
          or adverserial item that is difficult for computer vision to give a
          good suction position by performing haptic search. The idea of haptic
          search is that the smart suction cup have 4 chambers all with pressure
          sensors. So we can use the pressure readings to decide whether the
          suction is sucessful or not. And also if the suction is not fully
          successful, which direction of the robot arm should move to perform
          the next suction, and keep trying until we get it. In this way, even
          the computer vision is failing to give a good spot of scution, if we
          at least know where the item roughly is, we can still pick the items
          up successfully. In our case, since the 3D printed chess pieces have
          some holes and gaps due to the shape of the pieces, like the eyes and
          neck of the knights, we need to perform haptic search to get a good
          suction spot. Here is a <a href="">link</a> to the close up haptic
          search video in additional materials section. You can find more
          details about smart suction cup with haptic search
          <a href="https://arxiv.org/abs/2309.07360">here</a>
        </p>
      </div>
      <img class="noteImage" src="images/smart suction cup.png" />
      <div class="content">
        <h3>Software:</h3>
        <h4>Computer vision:</h4>
        <p>
          This part is straight forward, we use the color thresholding we
          learned in the lab to locate the center of the board by pasting a
          orange tape in the middle of the board. So we can create a grid
          mapping base on the center of the board. We first initialize the
          camera frame using the tool frame to get the transfrom from link to
          camera to get the position in base frame which is the world frame. The
          robot arm will first face the camera down and start detecting the
          orange tape in the camera frame, I then put the detected position in
          another python file so the robot arm will go to the center for us to
          verify whether it is accurate or not.
        </p>
      </div>
      <img class="noteImage" src="images/orange tape center.jpg" />
      <div class="content">
        <h4>Haptic Search and Pressure Sensing</h4>
        <p></p>
      </div>
      <img />
    </div>
  </body>
</html>
